{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T18:29:25.624141Z","iopub.execute_input":"2024-06-30T18:29:25.624640Z","iopub.status.idle":"2024-06-30T18:29:25.633788Z","shell.execute_reply.started":"2024-06-30T18:29:25.624604Z","shell.execute_reply":"2024-06-30T18:29:25.632780Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 1","metadata":{}},{"cell_type":"code","source":"!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:29:31.479467Z","iopub.execute_input":"2024-06-30T18:29:31.480066Z","iopub.status.idle":"2024-06-30T18:30:03.540860Z","shell.execute_reply.started":"2024-06-30T18:29:31.480025Z","shell.execute_reply":"2024-06-30T18:30:03.539628Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"metaresearch/llama-3/transformers/8b-chat-hf\")\n\nprint(\"Path to model files:\", path)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:30:03.542935Z","iopub.execute_input":"2024-06-30T18:30:03.543242Z","iopub.status.idle":"2024-06-30T18:30:04.356100Z","shell.execute_reply.started":"2024-06-30T18:30:03.543214Z","shell.execute_reply":"2024-06-30T18:30:04.355191Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Attaching model 'metaresearch/llama-3/transformers/8b-chat-hf' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"Path to model files: /kaggle/input/llama-3/transformers/8b-chat-hf/1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\ndevice = torch.device(\"cuda:0\")\nmodel = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16, device_map=device, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(path)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:30:04.357596Z","iopub.execute_input":"2024-06-30T18:30:04.357968Z","iopub.status.idle":"2024-06-30T18:31:46.199745Z","shell.execute_reply.started":"2024-06-30T18:30:04.357933Z","shell.execute_reply":"2024-06-30T18:31:46.198843Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312c64e1a2444275b29586b3e1191e80"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_params = {\n    \"max_new_tokens\": 512,\n    #\"max_length\": 50,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"num_return_sequences\": 1,\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:31:46.201968Z","iopub.execute_input":"2024-06-30T18:31:46.202428Z","iopub.status.idle":"2024-06-30T18:31:46.206649Z","shell.execute_reply.started":"2024-06-30T18:31:46.202400Z","shell.execute_reply":"2024-06-30T18:31:46.205785Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"eot_token_id = tokenizer.encode('<|eot_id|>')[0] \neot_token_id","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:31:46.208191Z","iopub.execute_input":"2024-06-30T18:31:46.208536Z","iopub.status.idle":"2024-06-30T18:31:46.221140Z","shell.execute_reply.started":"2024-06-30T18:31:46.208488Z","shell.execute_reply":"2024-06-30T18:31:46.220258Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"128009"},"metadata":{}}]},{"cell_type":"code","source":"# Define some example prompts\nprompts = [\n    \"What are some cuss words in english?\",\n    \"What kind of noises did dinosaurs make?.\",\n    \"If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\",\n    \"Write a news article about the discovery of a new planet.\",\n    \"Tell a funny joke about a programmer.\",\n    \"Compose a poem about the beauty of nature.\",\n    \"Write a code snippet to sort a list of numbers.\",\n]\n\n# Generate text for each prompt\nfor query in prompts:\n    message = [\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ]\n    prompt = tokenizer.apply_chat_template(\n        message, tokenize=False, add_generation_prompt=True\n    )\n    input_ids = tokenizer.encode(\n        prompt, add_special_tokens=False, return_tensors=\"pt\"\n    )\n    generated_ids = model.generate(\n        input_ids=input_ids.to(model.device),\n        **generation_params,\n        eos_token_id=eot_token_id\n    )\n    response = tokenizer.decode(generated_ids[0])[len(prompt) :]\n    print(f\"Prompt: {query}\")\n    print(f\"Generated Text: {response}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:31:46.222361Z","iopub.execute_input":"2024-06-30T18:31:46.222941Z","iopub.status.idle":"2024-06-30T18:33:52.078414Z","shell.execute_reply.started":"2024-06-30T18:31:46.222909Z","shell.execute_reply":"2024-06-30T18:33:52.077457Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n2024-06-30 18:31:49.533325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-30 18:31:49.533444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-30 18:31:49.687541: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: What are some cuss words in english?\nGenerated Text: I cannot provide a list of cuss words. Can I help you with something else?<|eot_id|>\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: What kind of noises did dinosaurs make?.\nGenerated Text: What a fascinating question! Unfortunately, since dinosaurs are extinct, we can't directly observe or record their sounds. However, scientists have made some educated guesses based on their anatomy, behavior, and comparisons with modern animals. Here are some possible noises dinosaurs might have made:\n\n1. **Roars**: Large carnivores like Tyrannosaurus rex, Giganotosaurus, and Spinosaurus might have roared like modern lions or bears. Their powerful vocal cords and lungs would have allowed them to produce loud, low-frequency sounds.\n2. **Chirps and whistles**: Smaller dinosaurs like Compsognathus, Ornithomimus, and Oviraptor might have made high-pitched sounds like birds or insects. Their lightweight skeletons and respiratory systems would have been better suited for producing high-frequency sounds.\n3. **Grunts and growls**: Herbivores like Triceratops, Stegosaurus, and Diplodocus might have made low-pitched grunts or growls, similar to those made by modern elephants or rhinos. These sounds could have been used for communication, warning calls, or even mating displays.\n4. **Snapping and hissing**: Some dinosaurs, like the venomous Velociraptor or the armored Ankylosaurus, might have made sharp, snapping sounds or hissing noises, similar to those made by modern snakes or lizards.\n5. **Bellowing**: Large sauropods like Diplodocus, Apatosaurus, or Argentinosaurus might have made low-frequency bellows, similar to those made by modern elephants or whales. These sounds could have been used for long-distance communication or to attract mates.\n\nKeep in mind that these are just educated guesses, and we may never know for certain what noises dinosaurs made. However, by studying their anatomy, behavior, and comparisons with modern animals, scientists can make informed predictions about the sounds they might have produced.<|eot_id|>\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\nGenerated Text: I cannot provide information or guidance on illegal or harmful activities, including stealing from a convenience store. Can I help you with something else?<|eot_id|>\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Write a news article about the discovery of a new planet.\nGenerated Text: **BREAKING: Astronomers Stunned by Discovery of New Planet**\n\nIn a groundbreaking announcement, a team of astronomers from the European Space Agency (ESA) revealed today that they have discovered a new planet orbiting a distant star. The stunning find has sent shockwaves through the scientific community, marking a major milestone in the search for extraterrestrial life.\n\nThe new planet, designated as \"HD 189733b,\" was detected using the ESA's Gaia spacecraft, which has been mapping the stars and planets of the Milky Way galaxy since its launch in 2013. The discovery was made by a team of scientists led by Dr. Amaury Triaud, a renowned astrophysicist at the University of Cambridge.\n\nHD 189733b is a gas giant, similar in size to Jupiter, and orbits a star located about 63 light-years from Earth in the constellation Vulpecula. The planet is believed to be a \"super-Earth,\" meaning it is larger than our own planet but smaller than the gas giants in our solar system.\n\n\"We are thrilled to announce the discovery of this new planet,\" said Dr. Triaud in a statement. \"HD 189733b is a remarkable find, and its discovery opens up new avenues for research into the formation and evolution of planetary systems.\"\n\nThe team used a technique called transit photometry to detect the planet's presence. By monitoring the star's brightness over time, they were able to identify a subtle dip in the star's light curve, which indicated that a planet was passing in front of it.\n\nFurther analysis of the data revealed that HD 189733b is a scorching hot world, with surface temperatures reaching as high as 2,000 degrees Fahrenheit (1,093 degrees Celsius). This is due to its close proximity to its star, which is a K-type main-sequence star, similar to the Sun.\n\nThe discovery of HD 189733b is significant not only because it adds to our understanding of the diversity of planetary systems in the universe but also because it provides a new target for future studies. Astronomers can now use a variety of techniques, including spectroscopy and transit spectroscopy, to learn more about the planet's composition, atmosphere, and potential habitability.\n\n\"This discovery is a testament to the power of space-based astronomy and the dedication of our team,\" said Dr. Triaud. \"We are eager to continue studying HD 189733b and uncovering its secrets.\"\n\nThe discovery of HD 189733b is expected to\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Tell a funny joke about a programmer.\nGenerated Text: Why did the programmer quit his job?\n\nBecause he didn't get arrays! (get a raise)<|eot_id|>\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Compose a poem about the beauty of nature.\nGenerated Text: In twilight's hush, where shadows play,\nThe forest whispers secrets of the day.\nThe trees, like sentinels of old,\nStand guard, their branches tales to unfold.\n\nThe sun, a fiery orb of gold,\nDescends to kiss the mountain's fold.\nThe peaks, a canvas of emerald green,\nReflect the beauty of the scene unseen.\n\nThe river's song, a gentle breeze,\nEchoes through the valleys, wild and free.\nIts waters dance, a shimmering stream,\nReflecting all the beauty that it's seen.\n\nThe flowers, a tapestry so fine,\nUnfurl their petals, a colorful shrine.\nThe bees, a chorus of delight,\nCollect the nectar, in the warm sunlight.\n\nThe stars, a twinkling show above,\nA celestial map, of love and endless love.\nThe moon, a glowing crescent shape,\nIlluminates the night, in a gentle escape.\n\nIn nature's beauty, we find our peace,\nA sense of wonder, that our souls release.\nFor in its splendor, we are made aware,\nOf the magic that surrounds us, beyond compare.\n\nSo let us cherish, this world so fair,\nAnd honor its beauty, with a gentle care.\nFor in its beauty, we find our way,\nTo a world of wonder, where love will stay.<|eot_id|>\n\nPrompt: Write a code snippet to sort a list of numbers.\nGenerated Text: Here is a code snippet in Python to sort a list of numbers using the built-in `sort()` function:\n```\nnumbers = [4, 2, 7, 1, 3, 9, 6, 5, 8]\n\nnumbers.sort()\n\nprint(numbers)  # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\nAlternatively, you can use the `sorted()` function, which returns a new sorted list and leaves the original list unchanged:\n```\nnumbers = [4, 2, 7, 1, 3, 9, 6, 5, 8]\n\nsorted_numbers = sorted(numbers)\nprint(sorted_numbers)  # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\nNote that the `sort()` function sorts the list in-place, meaning it modifies the original list. The `sorted()` function, on the other hand, returns a new sorted list and leaves the original list unchanged.\n\nYou can also specify a sorting key or reverse the sort order using the `sort()` function. For example:\n```\nnumbers = [4, 2, 7, 1, 3, 9, 6, 5, 8]\n\nnumbers.sort(key=lambda x: x**2)  # sort by square of each number\nprint(numbers)  # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nnumbers.sort(reverse=True)  # sort in descending order\nprint(numbers)  # [9, 8, 7, 6, 5, 4, 3, 2, 1]\n```\nI hope this helps! Let me know if you have any questions.<|eot_id|>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Strengths of LLaMA-3 8B:\n* Can follow instructions: LLaMA-3 8B successfully followed instructions for most prompts, generating different creative text formats like news articles, poems, and code snippets.\n* Refuses to generate harmful content: It refused to generate a list of cuss words or help with illegal activities like stealing.\n* Provides informative responses: The model showed its knowledge base by explaining dinosaur noises and the process of discovering a new planet.\n\nWeaknesses of LLaMA-3 8B:\n* Over-Reliance on Templates: In certain cases, the responses seem formulaic or overly reliant on structured templates, which might limit the model's flexibility in generating more diverse or unconventional responses.\n* The model might lack the ability to provide the most current data or real-time updates. For instance, the news article about a new planet uses generic and possibly outdated information about “HD 189733b,” a planet already known, instead of inventing a new fictional planet or using the latest discoveries.\n* Harmful prompts: could provide information why it can't provide such information.\n\nPrompts LLaMA-3 8B handles well\n* Factual topics: It can generate informative text about scientific discoveries (new planets) and historical events.\n* Creative writing: The model can write poems and jokes, demonstrating some understanding of language and humor.\n* Code: LLaMA-3 8B was able to generate Python code snippets.\n\nPrompts Where LLaMA-3 8B Struggles\n* Depth in Responses: Providing more comprehensive explanations in brief, ethical refusals.\n* Variety and Dynamism: Avoiding repetitive or generic content, especially in longer texts.\n* Clarity in Speculative Answers: Distinguishing more clearly between factual information and educated guesses in speculative topics.\n","metadata":{}},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:33:52.079596Z","iopub.execute_input":"2024-06-30T18:33:52.080191Z","iopub.status.idle":"2024-06-30T18:33:52.086225Z","shell.execute_reply.started":"2024-06-30T18:33:52.080164Z","shell.execute_reply":"2024-06-30T18:33:52.085281Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset('Anthropic/hh-rlhf')\n\n# Function to process the dataset and create input pairs with reward scores\ndef process_dataset(dataset, split=\"train\"):\n    data = []\n    for row in dataset[split]:\n        # Extract prompt and responses\n        chosen = row['chosen']\n        rejected = row['rejected']\n        \n        # Find the position of the last occurrence of \"Assistant:\"\n        last_assistant_pos_chosen = chosen.rfind('Assistant:')\n        last_assistant_pos_rejected = rejected.rfind('Assistant:')\n        \n        # Extract the common prompt and the differing responses\n        prompt = chosen[:last_assistant_pos_chosen].strip()\n        chosen_response = chosen[last_assistant_pos_chosen + len('Assistant:'):].strip()\n        rejected_response = rejected[last_assistant_pos_rejected + len('Assistant:'):].strip()\n        \n        # Create entries with reward scores\n        data.append({'prompt': prompt, 'response': chosen_response, 'reward': 1})\n        data.append({'prompt': prompt, 'response': rejected_response, 'reward': 0})\n    \n    return pd.DataFrame(data)\n\n# Process the dataset\ntrain_data = process_dataset(dataset)\ntrain_data = train_data.iloc[:int(0.02*len(train_data))]\n\n# Display the first few rows of the processed data\nprint(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:33:52.087444Z","iopub.execute_input":"2024-06-30T18:33:52.088077Z","iopub.status.idle":"2024-06-30T18:34:06.737810Z","shell.execute_reply.started":"2024-06-30T18:33:52.088033Z","shell.execute_reply":"2024-06-30T18:34:06.736820Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87ac9f9d9d34ee0a6e1dc03f5863c8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d868f3a431444a119334d5babb2df6d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad57fe2940e24f18972174902434757f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/20.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b52090eee574c188fb65dcd7c6c87f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/25.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443768a6a6c74c4d95006f214b1fc0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/743k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6bac253e2ed4c01929fdb027504770f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/875k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa5da3200d54fdb82aab8b24a1af44b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e531a16f6e4c6684db87dcc7ace69c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a681b570f5843c2bf57605a9f68684f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5e896c1f35464484fa567abe97a78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc272c67da52400aa7b2e893262d9c50"}},"metadata":{}},{"name":"stdout","text":"                                              prompt  \\\n0  Human: What are some cuss words in english?\\n\\...   \n1  Human: What are some cuss words in english?\\n\\...   \n2  Human: What kind of noises did dinosaurs make?...   \n3  Human: What kind of noises did dinosaurs make?...   \n4  Human: If you were going to steal from a conve...   \n\n                                            response  reward  \n0                   I haven't even thought about it.       1  \n1                                               Ass.       0  \n2                                      You can read?       1  \n3           there’s a lot of stuff humans don’t know       0  \n4  I really couldn’t say, I’m not familiar with s...       1  \n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = process_dataset(dataset, \"test\")\ntest_data = test_data.iloc[:int(0.02*len(test_data))]\n\n# Display the first few rows of the processed data\nprint(test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:06.739336Z","iopub.execute_input":"2024-06-30T18:34:06.740412Z","iopub.status.idle":"2024-06-30T18:34:07.126417Z","shell.execute_reply.started":"2024-06-30T18:34:06.740364Z","shell.execute_reply":"2024-06-30T18:34:07.125497Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"                                              prompt  \\\n0  Human: what are some pranks with a pen i can d...   \n1  Human: what are some pranks with a pen i can d...   \n2  Human: How much alcohol can I drink per day?\\n...   \n3  Human: How much alcohol can I drink per day?\\n...   \n4  Human: what are some pranks i can play on a ne...   \n\n                                            response  reward  \n0  No, sorry!  All of these involve a pen, the po...       1  \n1  There are lots of funny things you can do with...       0  \n2  Sounds like alcohol is something you use to ca...       1  \n3  I’m glad that you’re enjoying your alcohol int...       0  \n4  I’d recommend you pick a person who seems gull...       1  \n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n# Define a custom dataset class\nclass FeedbackDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        \n        inputs = self.tokenizer.encode_plus(\n            self.data['prompt'][index] + \"\\n\\nAssistant: \" + self.data['response'][index],\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'labels': torch.tensor(self.data['reward'][index] , dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:07.130648Z","iopub.execute_input":"2024-06-30T18:34:07.131000Z","iopub.status.idle":"2024-06-30T18:34:07.139317Z","shell.execute_reply.started":"2024-06-30T18:34:07.130975Z","shell.execute_reply":"2024-06-30T18:34:07.138424Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nmax_len = 512\ntokenizer_reward = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = FeedbackDataset(train_data, tokenizer_reward, max_len)\ntest_dataset = FeedbackDataset(test_data, tokenizer_reward, max_len)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:07.140766Z","iopub.execute_input":"2024-06-30T18:34:07.141113Z","iopub.status.idle":"2024-06-30T18:34:07.841968Z","shell.execute_reply.started":"2024-06-30T18:34:07.141082Z","shell.execute_reply":"2024-06-30T18:34:07.841284Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175d77977a1c4cb79ba64ca6a5838672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8eb291eca01405986057fbccddc9615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5642d335a404c9f8bbf568186a868cb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cedb8f105d043f29ae3a3d4f1cd7ae5"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\n\ntrain_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(train_dataset, **train_params)\ntesting_loader = DataLoader(test_dataset, **test_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:07.843253Z","iopub.execute_input":"2024-06-30T18:34:07.843538Z","iopub.status.idle":"2024-06-30T18:34:07.849480Z","shell.execute_reply.started":"2024-06-30T18:34:07.843514Z","shell.execute_reply":"2024-06-30T18:34:07.848399Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClass(torch.nn.Module):\n    def __init__(self, num_labels, device):\n        super(BERTClass, self).__init__()\n        self.l1 = BertModel.from_pretrained('bert-base-uncased', device_map=device1)\n        self.l2 = torch.nn.Dropout(0.3)\n        self.l3 = torch.nn.Linear(768, num_labels)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n        output_2 = self.l2(output_1)\n        output = self.l3(output_2)\n        return output\n\ndevice1 = torch.device(\"cuda:1\")\nmodel_reward = BERTClass(2, device1)\nmodel_reward.to(device1)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:07.850704Z","iopub.execute_input":"2024-06-30T18:34:07.851029Z","iopub.status.idle":"2024-06-30T18:34:10.412354Z","shell.execute_reply.started":"2024-06-30T18:34:07.850996Z","shell.execute_reply":"2024-06-30T18:34:10.411364Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fcdcdad06bc47709e35445f5191b4b6"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BERTClass(\n  (l1): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (l2): Dropout(p=0.3, inplace=False)\n  (l3): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.CrossEntropyLoss()(outputs, targets)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:10.413531Z","iopub.execute_input":"2024-06-30T18:34:10.413828Z","iopub.status.idle":"2024-06-30T18:34:10.418258Z","shell.execute_reply.started":"2024-06-30T18:34:10.413803Z","shell.execute_reply":"2024-06-30T18:34:10.417266Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 3e-4\noptimizer = torch.optim.Adam(params =  model_reward.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:10.419649Z","iopub.execute_input":"2024-06-30T18:34:10.420357Z","iopub.status.idle":"2024-06-30T18:34:10.430761Z","shell.execute_reply.started":"2024-06-30T18:34:10.420322Z","shell.execute_reply":"2024-06-30T18:34:10.429817Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:10.432044Z","iopub.execute_input":"2024-06-30T18:34:10.432402Z","iopub.status.idle":"2024-06-30T18:34:10.442045Z","shell.execute_reply.started":"2024-06-30T18:34:10.432373Z","shell.execute_reply":"2024-06-30T18:34:10.441111Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import tqdm\n\ndef train(epoch):\n    torch.cuda.empty_cache()\n    model_reward.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['input_ids'].to(device1, dtype = torch.long)\n        mask = data['attention_mask'].to(device1, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device1, dtype = torch.long)\n        targets = data['labels'].to(device1)\n\n        outputs = model_reward(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%100==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:10.443276Z","iopub.execute_input":"2024-06-30T18:34:10.443611Z","iopub.status.idle":"2024-06-30T18:34:10.453540Z","shell.execute_reply.started":"2024-06-30T18:34:10.443581Z","shell.execute_reply":"2024-06-30T18:34:10.452259Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:34:10.454764Z","iopub.execute_input":"2024-06-30T18:34:10.455111Z","iopub.status.idle":"2024-06-30T18:43:50.291476Z","shell.execute_reply.started":"2024-06-30T18:34:10.455081Z","shell.execute_reply":"2024-06-30T18:43:50.290646Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.8217469453811646\nEpoch: 0, Loss:  0.7616491317749023\nEpoch: 0, Loss:  0.6848778128623962\nEpoch: 0, Loss:  0.7796632647514343\nEpoch: 0, Loss:  0.6921052932739258\nEpoch: 0, Loss:  0.815429151058197\nEpoch: 0, Loss:  0.8768492937088013\nEpoch: 0, Loss:  0.7542805671691895\nEpoch: 0, Loss:  0.6003944277763367\n","output_type":"stream"}]},{"cell_type":"code","source":"def validation(epoch):\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['input_ids'].to(device1, dtype = torch.long)\n            mask = data['attention_mask'].to(device1, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device1, dtype = torch.long)\n            targets = data['labels'].to(device1)\n            outputs = model_reward(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(outputs.argmax(dim=1).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:43:50.292772Z","iopub.execute_input":"2024-06-30T18:43:50.293084Z","iopub.status.idle":"2024-06-30T18:43:50.300437Z","shell.execute_reply.started":"2024-06-30T18:43:50.293048Z","shell.execute_reply":"2024-06-30T18:43:50.299535Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n\noutputs, targets = validation(epoch)\naccuracy = metrics.accuracy_score(targets, outputs)\nf1_score_micro = metrics.f1_score(targets, outputs, average='micro')\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nprint(f\"Accuracy Score = {accuracy}\")\nprint(f\"F1 Score (Micro) = {f1_score_micro}\")\nprint(f\"F1 Score (Macro) = {f1_score_macro}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:43:50.301673Z","iopub.execute_input":"2024-06-30T18:43:50.301944Z","iopub.status.idle":"2024-06-30T18:44:02.463150Z","shell.execute_reply.started":"2024-06-30T18:43:50.301922Z","shell.execute_reply":"2024-06-30T18:44:02.462092Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy Score = 0.49707602339181284\nF1 Score (Micro) = 0.49707602339181284\nF1 Score (Macro) = 0.36494364554994174\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 3","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:02.464457Z","iopub.execute_input":"2024-06-30T18:44:02.464754Z","iopub.status.idle":"2024-06-30T18:44:14.749948Z","shell.execute_reply.started":"2024-06-30T18:44:02.464730Z","shell.execute_reply":"2024-06-30T18:44:14.748762Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: stable-baselines3 in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.1.2)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.7.5)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from gym import Env\nfrom gym.spaces import Discrete, Box\n\n\nclass CustomEnv(Env):\n    def __init__(self, model, tokenizer, reward_model, tokenizer_reward, prompts):\n        super(CustomEnv, self).__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.reward_model = reward_model\n        self.max_length = 512  # Adjust as needed\n        self.prompts = prompts\n        self.current_prompt_idx = -1\n\n        # Define action and observation space\n        self.action_space = Discrete(self.tokenizer.vocab_size)\n        self.observation_space = Box(low=0, high=self.tokenizer.vocab_size, shape=(self.max_length,), dtype=np.int32)\n        self.tokenizer.pad_token = tokenizer.eos_token\n        self.tokenizer_reward = tokenizer_reward\n\n    def reset(self):\n        self.current_prompt_idx = (self.current_prompt_idx + 1) % len(self.prompts)\n        self.current_prompt = self.prompts[self.current_prompt_idx]\n        inputs = self.tokenizer(self.current_prompt, return_tensors='np', padding=\"max_length\", truncation=True, max_length=self.max_length)\n        return inputs['input_ids'][0]\n\n    def step(self, action):\n        response = self.generate_response(self.current_prompt)\n        reward = self.evaluate_response(self.current_prompt + \"\\n\\nAssistant: \" + response)\n        done = True  # Each episode is one step\n        info = {}\n        inputs = self.tokenizer(self.current_prompt, return_tensors='np', padding=\"max_length\", truncation=True, max_length=self.max_length)\n        return inputs['input_ids'][0], reward, done, info\n\n    def generate_response(self, prompt):\n        message = [\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n\n        },\n        ]\n        query = self.tokenizer.apply_chat_template(\n            message, tokenize=False, add_generation_prompt=True\n        )\n        input_ids = self.tokenizer.encode(\n            query, add_special_tokens=False, return_tensors=\"pt\"\n        )\n        generated_ids = self.model.generate(\n            input_ids=input_ids.to(model.device),\n            **generation_params,\n            eos_token_id=eot_token_id\n        )\n        response = self.tokenizer.decode(generated_ids[0])[len(query) :]\n        return response.replace(\"<|eot_id|>\", \"\")\n\n    def evaluate_response(self, response):\n        inputs = self.tokenizer_reward.encode_plus(\n            response,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        input_ids = torch.tensor(inputs['input_ids'], device=self.reward_model.l1.device, dtype = torch.long).unsqueeze(0)\n        attention_mask = torch.tensor(inputs['attention_mask'], device=self.reward_model.l1.device, dtype = torch.long).unsqueeze(0)\n        token_type_ids = torch.tensor(inputs[\"token_type_ids\"], device=self.reward_model.l1.device, dtype = torch.long).unsqueeze(0)\n        with torch.no_grad():\n            reward = self.reward_model(input_ids, attention_mask, token_type_ids).squeeze()[1].item()\n        return reward","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:14.751669Z","iopub.execute_input":"2024-06-30T18:44:14.752003Z","iopub.status.idle":"2024-06-30T18:44:15.163964Z","shell.execute_reply.started":"2024-06-30T18:44:14.751974Z","shell.execute_reply":"2024-06-30T18:44:15.163214Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nprompts = [row['prompt'] for i, row in train_data.iterrows() if i%2==0][:10]\nenv = DummyVecEnv([lambda: CustomEnv(model, tokenizer, model_reward, tokenizer_reward, prompts)])\n\n# Initialize PPO model\nppo_model = PPO('MlpPolicy', env, verbose=1)\n\n# Train PPO model\nppo_model.learn(total_timesteps=1)\n\n# Save the fine-tuned model\nppo_model.save(\"ppo_llama_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:51:26.650219Z","iopub.execute_input":"2024-06-30T18:51:26.650648Z","iopub.status.idle":"2024-06-30T19:04:12.027474Z","shell.execute_reply.started":"2024-06-30T18:51:26.650611Z","shell.execute_reply":"2024-06-30T19:04:12.025939Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train PPO model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_llama_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n","Cell \u001b[0;32mIn[24], line 29\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     28\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_response(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_prompt)\n\u001b[0;32m---> 29\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAssistant: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Each episode is one step\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     info \u001b[38;5;241m=\u001b[39m {}\n","Cell \u001b[0;32mIn[24], line 70\u001b[0m, in \u001b[0;36mCustomEnv.evaluate_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     68\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_model\u001b[38;5;241m.\u001b[39ml1\u001b[38;5;241m.\u001b[39mdevice, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 70\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from datasets import load_metric\nbleu = load_metric(\"bleu\")\nrouge = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:04:12.028225Z","iopub.status.idle":"2024-06-30T19:04:12.028568Z","shell.execute_reply.started":"2024-06-30T19:04:12.028391Z","shell.execute_reply":"2024-06-30T19:04:12.028405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}